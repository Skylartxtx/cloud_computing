version: "3.3"

volumes:
    project_shared_workspace:
    project_hadoop_datanode1:
    project_hadoop_datanode2:
    project_hadoop_datanode3:
    project_hadoop_namenode:
    project_hadoop_historyserver:

networks:
    project-spark-net:
        driver: bridge

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: project_namenode
    networks:
      - "project-spark-net"
    ports:
      - 50070:9870  # HDFS Web UI (调整为 50070 以避免与 cca3 冲突)
      - 9001:9000   # HDFS 服务端口 (调整为 9001)
    volumes:
      - project_hadoop_namenode:/hadoop/dfs/name
      - ./project:/home/project
    environment:
      - CLUSTER_NAME=project_cluster
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: project_resourcemanager
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    networks:
      - "project-spark-net"
    ports:
      - 8090:8088   # ResourceManager UI (调整为 8090)
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: project_historyserver
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    networks:
      - "project-spark-net"
    volumes:
      - project_hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: project_nodemanager1
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    networks:
      - "project-spark-net"
    env_file:
      - ./hadoop.env

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: project_datanode1
    depends_on:
      - namenode
    networks:
      - "project-spark-net"
    volumes:
      - project_hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: project_datanode2
    depends_on:
      - namenode
    networks:
      - "project-spark-net"
    volumes:
      - project_hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  datanode3:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: project_datanode3
    depends_on:
      - namenode
    networks:
      - "project-spark-net"
    volumes:
      - project_hadoop_datanode3:/hadoop/dfs/data
    env_file:
      - ./hadoop.env

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: project_spark-master
    depends_on:
      - namenode
    networks:
      - "project-spark-net"
    ports:
      - "8083:8080"  # Spark Master UI (调整为 8083)
      - "7078:7077"  # Spark Master 端口 (调整为 7078)
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - NAMENODE_HOSTNAME=project_namenode

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: project_spark-worker-1
    depends_on:
      - spark-master
      - namenode
    networks:
      - "project-spark-net"
    ports:
      - "8084:8081"  # Spark Worker 1 UI (调整为 8084)
    environment:
      - "SPARK_MASTER=spark://project_spark-master:7078"
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
      - NAMENODE_HOSTNAME=project_namenode

  spark-worker-2:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: project_spark-worker-2
    depends_on:
      - spark-master
      - namenode
    networks:
      - "project-spark-net"
    ports:
      - "8085:8082"  # Spark Worker 2 UI (调整为 8085)
    environment:
      - "SPARK_MASTER=spark://project_spark-master:7078"
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
      - NAMENODE_HOSTNAME=project_namenode

  jupyter-notebook:
    container_name: project_jupyter
    image: jupyter/all-spark-notebook:42f4c82a07ff
    depends_on:
      - spark-master
    expose:
      - "8889"
    networks:
      - "project-spark-net"
    user: root
    ports:
      - "8889:8888"  # Jupyter Notebook (调整为 8889)
    volumes:
      - ./project:/home/jovyan/work/project
      - ./events:/tmp/spark-events
    command: bash -c "pip install gdown faiss-cpu && cp /home/jovyan/work/project/cardio_train.csv /home/jovyan/work/project && start-notebook.sh --ip=0.0.0.0 --allow-root --NotebookApp.token=''"

